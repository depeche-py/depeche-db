{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Depeche DB","text":"<p>A library for building event-based systems on top of PostgreSQL</p>"},{"location":"#features","title":"Features","text":"<p>Depeche DB has two main parts:</p>"},{"location":"#message-store","title":"Message store","text":"<ul> <li>store &amp; read messages</li> <li>multiple streams</li> <li>optimistic concurrency control</li> </ul>"},{"location":"#subscriptions","title":"Subscriptions","text":"<ul> <li>aggregate messages from multiple streams</li> <li>subscribe to aggregated stream</li> <li>no polling (uses PostgreSQL <code>NOTIFY</code>/<code>LISTEN</code>)</li> <li>tools for keeping track of subscription state</li> <li>concurrent processing (aggregated streams can be partioned)</li> </ul>"},{"location":"#background","title":"Background","text":"<p>Depeche DB is obviously influenced by message stores like EventStoreDB and takes some inspiration from Kafka. Another strong influence has been the Message DB project which is pretty similar on the implementation side.</p>"},{"location":"concepts/data-model/","title":"Data model","text":""},{"location":"concepts/data-model/#messages","title":"Messages","text":"<p>The smallest unit of storage is a message. The content of messages is transparent to Depeche DB. Every message needs two basic properties so that it can be stored and processed: An <code>ID</code> of type UUID and a timestamp.</p>"},{"location":"concepts/data-model/#streams","title":"Streams","text":"<p>A stream has a name and contains messages. The message store can contain as many streams as you need.</p> <p>flowchart TB     subgraph stream-1     msg11 --&gt; msg12 --&gt; msg13     end  flowchart TB     subgraph stream-2     msg21 --&gt; msg22     end </p> <p>Messages in a stream are ordered. Their position within the stream is called <code>version</code>. The <code>version</code>s in a stream do not have gaps and are unique.</p> <p>Usually, a stream will be relatively small because it only contains messages concerning a single domain object.</p>"},{"location":"concepts/data-model/#message-store","title":"Message store","text":"<p>The message store contains multiple streams. Messages are assigned a global position when they are added to a stream. This global position is unique but there can be gaps (when transactions are rolled back). For messages <code>a</code> and <code>b</code> within the same stream, it is guaranteed that if <code>a.version &lt; b.version</code> then <code>a.global_position &lt; b.global_position</code> and vice versa.</p>"},{"location":"concepts/data-model/#aggregated-streams","title":"Aggregated streams","text":"<p>Because of the high granularity of the streams, it is not pratical to subscribe to them directly. That is why there is the notion of an aggregated stream which aggregates the messages from several streams.</p> <p>Aggregated streams consist of partitions. Messages from the origin streams can be assigned to partitions by a user defined function. When a message is added to a partition within an aggregated stream, it is not copied but linked to. Thus, aggregated streams are rather light-weight on the storage size.</p> <p>Given these streams: flowchart TB     subgraph stream-1     msg11 --&gt; msg12 --&gt; msg13     style msg11 fill:#feaaaa     style msg12 fill:#feaaaa     style msg13 fill:#feaaaa     end     subgraph stream-2     msg21 --&gt; msg22     style msg21 fill:#aafeaa     style msg22 fill:#aafeaa     end     subgraph stream-3     msg31 --&gt; msg32     style msg31 fill:#aaaafe     style msg32 fill:#aaaafe     end </p> <p>An aggregated stream could look like this: flowchart TB     subgraph partition-1       p11 --&gt; p12 --&gt; p31 --&gt; p13 --&gt; p32       style p11 fill:#feaaaa       style p12 fill:#feaaaa       style p13 fill:#feaaaa       style p31 fill:#aaaafe       style p32 fill:#aaaafe     end  flowchart TB     subgraph partition-2       p21 --&gt; p22       style p21 fill:#aafeaa       style p22 fill:#aafeaa     end </p> <p>Within a partition, messages are ordered by their global position in the message store. Messages are assigned a unique &amp; gapless <code>position</code> within their partition.</p> <p>Please note that this example partitioning (which used the stream name as the partition key) is only one possibility. Users can define how messages are partitioned. Furthermore, you can create multiple aggregated streams which contain the same or overlapping sets of messages with different partitioning strategies. This allows you to tailor the aggregated streams - more specifically their ordering and possible parallelism - to the needs of different consumers.</p>"},{"location":"concepts/subscriptions/","title":"Subscriptions","text":"<p>Given an aggregated stream, we can create a number of subscriptions on it which are discriminated by a name. A subscription works roughly like this:</p> <ol> <li>Wait for a trigger</li> <li>Get the current state (pairs of partition number &amp; last processed position)</li> <li>For each partition that has messages after the known position (ordered by the    message time of the oldest unprocessed message)<ol> <li>Try to acquire a lock (subscription group name, partition)</li> <li>Re-validate the state (a parallel process might have advanced it already)</li> <li>Process the message(s) (depending on the batch size)</li> <li>Update the state with the new position</li> <li>Release the lock</li> </ol> </li> </ol> <p>This algorithm allows us to process messages concurrently while still honoring the ordering guarantees within a partition because only one instance can process messages from any given partition at any time. It can be described as an instance of the competing consumers pattern.</p> <p>The ordering done in step 3 is not necessary to keep any of the guarantees but helps to keep up with the expectation that messages in the system are processed roughly in the order given by their message time. It also helps with fairness, because it will favor processing older messages first.</p> <p>The order of steps 3c and 3d makes this a \"at least once\" delivery system, because the message is processed before the new position is recorded.</p>"},{"location":"concepts/subscriptions/#services-required-by-subscriptions","title":"Services required by subscriptions","text":"<p>A subscription requires two services provided to it:</p> <ol> <li>Subscription state storage</li> <li>(Distributed) Locks</li> </ol> <p>Both of the services have a default implementation but it may be a good idea to check if you should customize/replace it to your needs. The interfaces of both services are pretty simple.</p>"},{"location":"concepts/subscriptions/#state","title":"State","text":"<p>The best location for the subscription state is - especially in the context of transactionally safe storage - close to the state that is altered in the course of the application code handling messages.</p> <p>If you can manage to get the subscription state updates within the same transaction as the application state, you essentially implemented a \"exactly-once\" message processing mechanism.</p>"},{"location":"concepts/subscriptions/#locks","title":"Locks","text":"<p>The default locking provider uses again PostgreSQL, namely advisory locks through the <code>pals</code> library. If for some reason your system is not actually distributed to multiple machines, you could replace if with a simpler &amp; faster locking system. Or maybe you already have a distributed locking infrastructure, then it might be worth a look to use it here too.</p>"},{"location":"getting-started/aggregated-stream/","title":"Aggregated stream","text":"<p>We will use the same message store as in the previous chapter here, but we will create a new set of streams within it:</p> <pre><code>import random\nfor _ in range(20):\nn = random.randint(0, 200)\nstream = f\"aggregate-me-{n % 5}\"\nmessage_store.write(stream=stream, message=EventA(num=n))\n</code></pre> <p>For our aggregated stream, we need to prepare a partition function (or rather class).</p> <pre><code>from depeche_db import StoredMessage\nclass NumMessagePartitioner:\ndef get_partition(self, message: StoredMessage[EventA]) -&gt; int:\nreturn message.message.num % 3\n</code></pre> <p>Now we can put together the aggregated stream.</p> <pre><code>from depeche_db import LinkStream, StreamProjector\nlink_stream = LinkStream(\nname=\"example_docs_aggregate_me\",\nstore=message_store,\n)\nstream_projector = StreamProjector(\nstream=link_stream,\npartitioner=NumMessagePartitioner(),\nstream_wildcards=[\"aggregate-me-%\"],\n)\nstream_projector.update_full()\n</code></pre> <p>Whenever we call <code>update_full</code>, all new messages in the origin streams will be appended to the relevant partition of the aggregated stream in the right order. We will not have to call this manually, but we can use the <code>TODO executor?!</code>.</p> <p>Now let's read from the aggregated stream. (We would usually not to this, because we rather consume aggregated stream through subscriptions.)</p> <pre><code>result = next(link_stream.read(conn=db_engine.connect(), partition=0))\nprint(result)\n# 4680cbaf-977e-43a4-afcb-f88e92043e9c (this is the message ID of the first message in partition 0)\nwith message_store.reader() as reader:\nprint(reader.get_message_by_id(result))\n# StoredMessage(\n#     message_id=UUID(\"4680cbaf-977e-43a4-afcb-f88e92043e9c\"),\n#     stream=\"aggregate-me-0\",\n#     ...\n# )\n</code></pre>"},{"location":"getting-started/executor/","title":"Executor","text":"<p>Executing the regular jobs of</p> <ul> <li>updating aggregated streams</li> <li>running handlers on subscriptions</li> </ul> <p>by hand is cumbersome.</p> <p>Here is how we would do this:</p> <pre><code>from depeche_db import Executor\nexecutor = Executor(db_dsn=DB_DSN)\nexecutor.register(stream_projector)\nexecutor.register(my_handler)\n# this will run until stopped via SIGINT etc\nexecutor.run()\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Install from PyPI using your favorite package manager</p> <pre><code>pip install depeche-db\n# OR\npoetry add depeche-db\n</code></pre>"},{"location":"getting-started/installation/#optional-run-a-postgresql-database","title":"Optional: Run a PostgreSQL database","text":"<pre><code># docker-compose.yml\nversion: '3'\nservices:\ndb_dev:\nimage: 'postgres:14.5'\nenvironment:\nPOSTGRES_USER: demo\nPOSTGRES_PASSWORD: demo\nPOSTGRES_DB: demo\nports:\n- 4888:5432\nrestart: unless-stopped\n</code></pre> <pre><code>docker compose up -d\n</code></pre>"},{"location":"getting-started/subscription/","title":"Subscription","text":"<p>Given the aggregated stream from the previous chapter, we can put together a subscription.</p> <pre><code>from depeche_db import Subscription\nfrom depeche_db.tools import DbLockProvider, DbSubscriptionStateProvider\nsubscription = Subscription(\ngroup_name=\"sub_example_docs_aggregate_me\",\nstream=link_stream,\nstate_provider=DbSubscriptionStateProvider(\nname=\"sub_state1\",\nengine=db_engine,\n),\nlock_provider=DbLockProvider(name=\"locks1\", engine=db_engine),\n)\n</code></pre> <p>You can read from a subscription directly (but you would probably want to use a <code>SubscriptionHandler</code> as shown next). The emitted message is wrapped in a <code>SubscriptionMessage</code> object which contains the metadata about the message in the context of the subscription/aggregated stream.</p> <pre><code>with subscription.get_next_message() as message:\nprint(message)\nmessage.ack()\n# SubscriptionMessage(\n#     partition=2,\n#     position=0,\n#     stored_message=StoredMessage(...)\n# )\n</code></pre> <p>In order to really handle the message on a subscription we would use this instead:</p> <pre><code>from depeche_db import SubscriptionHandler, SubscriptionMessage\nmy_handler = SubscriptionHandler(\nsubscription=subscription,\n)\n@my_handler.register\ndef handle_event_a(message: SubscriptionMessage[EventA]):\nreal_message = message.stored_message.message\nprint(\nf\"Got EventA: {real_message.num} on partition {message.partition} at {message.position}\"\n)\nmy_handler.run_once()\n# Got EventA: 150 on partition 0 at 0\n# Got EventA: 175 on partition 1 at 0\n# Got EventA: 94 on partition 1 at 1\n# Got EventA: 89 on partition 2 at 1\n# Got EventA: 73 on partition 1 at 2\n# Got EventA: 66 on partition 0 at 1\n# ...\n</code></pre>"},{"location":"getting-started/write-read/","title":"Writing &amp; reading messages","text":"<p>First, create a SQLAlchemy engine with your database connection:</p> <pre><code>from sqlalchemy import create_engine\nDB_DSN = \"postgresql://demo:demo@localhost:4888/demo\"\ndb_engine = create_engine(DB_DSN)\n</code></pre> <p>Then we define our message types using pydantic. Using pydantic is optional, but it makes serialization straightforward.</p> <pre><code>from datetime import datetime\nfrom uuid import UUID, uuid4\nimport pydantic\nfrom depeche_db import MessageProtocol\nclass MyEvent(pydantic.BaseModel, MessageProtocol):\nevent_id: UUID = pydantic.Field(default_factory=uuid4)\nhappened_at: datetime = pydantic.Field(default_factory=datetime.utcnow)\ndef get_message_id(self) -&gt; UUID:\nreturn self.event_id\ndef get_message_time(self) -&gt; datetime:\nreturn self.happened_at\nclass EventA(MyEvent):\nnum : int\nclass EventB(MyEvent):\ntext : str\n</code></pre> <p>Now we are ready to create our message store. This will create a new table <code>example_basic_messages</code> when it is called the first time.</p> <pre><code>from depeche_db import MessageStore\nfrom depeche_db.tools import PydanticMessageSerializer\nmessage_store = MessageStore(\nname=\"example_basic\",\nengine=db_engine,\nserializer=PydanticMessageSerializer(EventA | EventB),\n)\n</code></pre> <p>Now we write an event to the stream</p> <pre><code>stream = f\"stream-{uuid4()}\"\nresult = message_store.write(stream=stream, message=EventA(num=42))\nprint(result)\n# MessagePosition(stream='stream-&lt;uuid&gt;', version=1, global_position=1)\n</code></pre> <p>Here is how we can read the messages:</p> <pre><code>print(next(message_store.read(stream)))\n# StoredMessage(\n#   message_id=UUID('&lt;uuid&gt;'),\n#   stream='stream-&lt;uuid&gt;',\n#   version=1,\n#   message=EventA(\n#     event_id=UUID('&lt;uuid&gt;'),\n#     num=42,\n#     happened_at=datetime.datetime(...)\n#   ),\n#   global_position=1\n# )\n</code></pre> <p>Please note that when reading, the original message is wrapped in a <code>StoredMessage</code> object, which contains the metadata about the message.</p> <p>When we write, we can pass an <code>expected_version</code> parameter, which gives us optimistic concurrency control. <pre><code># this fails because the expected version is 0, but the stream already has a message\nmessage_store.write(stream=stream, message=EventA(num=23), expected_version=0)\n# this is fine, because we expect the right version\nmessage_store.write(stream=stream, message=EventA(num=23), expected_version=1)\n</code></pre></p>"}]}