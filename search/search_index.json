{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Depeche DB","text":"<p>Depeche DB is modern Python library for building event-based systems</p> <p>Key features:</p> <ul> <li>Message store with optimistic concurrency control &amp; strong ordering guarantees</li> <li>Subscriptions with \"at least once\" semantics</li> <li>Parallel processing of (partitioned) subscriptions</li> <li>No database polling</li> </ul>"},{"location":"#use-cases","title":"Use cases","text":"<p>Depeche DB can be used to cover the following:</p> <ul> <li>Building event-sourced systems</li> <li>Building pub-sub systems</li> <li>Asynchronous command handling</li> <li>Use as a transactional outbox when application state and event store need to be in-sync</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>If you are interested in a quick tour of the features shown with example code, please have a look at the Getting Started Guide.</p> <p>More details on the data model and algorithms can be found on the Concepts pages.</p> <p>The API Docs give you all the details on how to use the library.</p>"},{"location":"#justification","title":"Justification","text":"<p>You might ask \"Why on earth use PostgreSQL to store events?\". For many use cases, you are probably right to assume that this is not the best solution.</p> <p>The main driver behind the development of this library is simplicity. We want to use boring tools and benefit from the knowledge built over years. With a RDBMS like PostgreSQL, you know what you get. You have proven ways of scaling them. You know how a backup works. You known their limits.</p> <p>So, if an application does not generate or handle hundreds of events per second and will not accumulate millions of messages, maybe using a RDBMS might be a good choice.</p>"},{"location":"#background-prior-art","title":"Background &amp; prior art","text":"<p>Depeche DB is obviously influenced by message stores like EventStoreDB and takes some inspiration from Kafka. More inspiration is taken from Marten, a project that also implements an event store on top of PostgreSQL. Another strong influence has been the Message DB project which is pretty similar on the implementation side.</p>"},{"location":"CHANGELOG/","title":"0.7.0","text":"<ul> <li>Add <code>start_point</code> to subscriptions</li> <li>Add three implementations for the subscription start point<ul> <li>Default: Beginning of aggregated stream</li> <li>Next message</li> <li>Point in time</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#062","title":"0.6.2","text":"<ul> <li>Add <code>register_manual</code> method to <code>MessageHandlerRegister</code></li> </ul>"},{"location":"CHANGELOG/#061","title":"0.6.1","text":"<ul> <li>Add small event sourcing lib</li> </ul>"},{"location":"CHANGELOG/#060","title":"0.6.0","text":"<ul> <li>Nicer interface to create <code>AggregatedStream</code> &amp; <code>Subscription</code></li> <li>Move runner back onto <code>Subscription</code></li> <li>Improved exceptions</li> </ul>"},{"location":"CHANGELOG/#050","title":"0.5.0","text":"<ul> <li>Fix excessive CPU usage caused by <code>threading.Event.wait</code></li> <li>Add API docs</li> <li>Split <code>SubscriptionHandler</code> into 3 classes and invert dependency to <code>Subscription</code><ul> <li><code>MessageHandlerRegister</code> allows registering and retrieving handlers using type hints</li> <li><code>SubscriptionMessageHandler</code> uses register to message handling (including call middleware &amp; error handling)</li> <li><code>SubscriptionRunner</code> allows contionously running the message handler on a <code>Subscription</code></li> </ul> </li> <li>Add class-based <code>MessageHandler</code> which wraps a <code>MessageHandlerRegister</code> and   implements the same interface</li> </ul>"},{"location":"CHANGELOG/#045","title":"0.4.5","text":"<ul> <li>Expose <code>RunOnNotification</code> and <code>CallMiddleware</code></li> </ul>"},{"location":"api/aggregated_stream/","title":"AggregatedStream","text":""},{"location":"api/aggregated_stream/#depeche_db.AggregatedStreamFactory","title":"<code>depeche_db.AggregatedStreamFactory(store)</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>This factory is accessible on the message store:</p> <pre><code>store = MessageStore(...)\nstream = store.aggregated_stream(\n    name=\"stream_name\",\n    partitioner=...,\n    stream_wildcards=[\"stream_%\"]\n)\n</code></pre>"},{"location":"api/aggregated_stream/#depeche_db.AggregatedStreamFactory.__call__","title":"<code>__call__(name, partitioner, stream_wildcards, update_batch_size=None)</code>","text":"<p>Create an aggregated stream.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the stream, needs to be a valid python identifier</p> required <code>partitioner</code> <code>MessagePartitioner[E]</code> <p>A partitioner for the stream</p> required <code>stream_wildcards</code> <code>List[str]</code> <p>A list of stream wildcards</p> required <code>update_batch_size</code> <code>Optional[int]</code> <p>The batch size for updating the stream</p> <code>None</code>"},{"location":"api/aggregated_stream/#depeche_db.AggregatedStream","title":"<code>depeche_db.AggregatedStream(name, store, partitioner, stream_wildcards, update_batch_size=None)</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>AggregatedStream aggregates multiple streams into one (partitioned) stream.</p> <p>Read more about aggregated streams under Data Model.</p> <p>The <code>update_batch_size</code> argument can be used to control the batch size of the update process. Higher numbers will result in less database roundtrips but also in higher memory usage.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Stream name, needs to be a valid python identifier</p> required <code>store</code> <code>MessageStore[E]</code> <p>Message store</p> required <code>partitioner</code> <code>MessagePartitioner[E]</code> <p>Message partitioner</p> required <code>stream_wildcards</code> <code>List[str]</code> <p>List of stream wildcards</p> required <code>update_batch_size</code> <code>Optional[int]</code> <p>Batch size for updating the stream, defaults to 100</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Stream name</p> <code>projector</code> <code>StreamProjector</code> <p>Stream projector</p> <code>subscription</code> <code>SubscriptionFactory</code> <p>Factory to create subscriptions on this stream</p>"},{"location":"api/aggregated_stream/#depeche_db.AggregatedStream.get_partition_statistics","title":"<code>get_partition_statistics(position_limits=None, result_limit=None)</code>","text":"<p>Get partition statistics for deciding which partitions to read from. This is used by subscriptions.</p>"},{"location":"api/aggregated_stream/#depeche_db.AggregatedStream.global_position_to_positions","title":"<code>global_position_to_positions(global_position)</code>","text":"<p>Get the positions for each partition at a given global position.</p>"},{"location":"api/aggregated_stream/#depeche_db.AggregatedStream.read","title":"<code>read(partition)</code>","text":"<p>Read all messages from a partition of the aggregated stream.</p> <p>Parameters:</p> Name Type Description Default <code>partition</code> <code>int</code> <p>Partition number</p> required"},{"location":"api/aggregated_stream/#depeche_db.AggregatedStream.read_slice","title":"<code>read_slice(partition, start, count)</code>","text":"<p>Read a slice of messages from a partition of the aggregated stream.</p> <p>Parameters:</p> Name Type Description Default <code>partition</code> <code>int</code> <p>Partition number</p> required <code>start</code> <code>int</code> <p>Start position</p> required <code>count</code> <code>int</code> <p>Number of messages to read</p> required"},{"location":"api/aggregated_stream/#depeche_db.AggregatedStream.time_to_positions","title":"<code>time_to_positions(time)</code>","text":"<p>Get the positions for each partition at a given time.</p>"},{"location":"api/aggregated_stream/#depeche_db.AggregatedStream.truncate","title":"<code>truncate(conn)</code>","text":"<p>Truncate aggregated stream.</p>"},{"location":"api/aggregated_stream/#depeche_db.StreamProjector","title":"<code>depeche_db.StreamProjector(stream, partitioner, stream_wildcards, batch_size=None)</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>Stream projector is responsible for updating an aggregated stream.</p> <p>The update process is locked to prevent concurrent updates. Thus, it is fine to run the projector in multiple processes.</p> <p>Implements: RunOnNotification</p>"},{"location":"api/aggregated_stream/#depeche_db.StreamProjector.notification_channel","title":"<code>notification_channel: str</code>  <code>property</code>","text":"<p>Returns the notification channel name for this projector.</p>"},{"location":"api/aggregated_stream/#depeche_db.StreamProjector.run","title":"<code>run()</code>","text":"<p>Runs the projector once.</p>"},{"location":"api/aggregated_stream/#depeche_db.StreamProjector.stop","title":"<code>stop()</code>","text":"<p>No-Op on this class.</p>"},{"location":"api/aggregated_stream/#depeche_db.StreamProjector.update_full","title":"<code>update_full()</code>","text":"<p>Updates the projection from the last known position to the current position.</p>"},{"location":"api/aggregated_stream/#depeche_db.MessagePartitioner","title":"<code>depeche_db.MessagePartitioner</code>","text":"<p>             Bases: <code>Protocol</code>, <code>Generic[E]</code></p> <p>Message partitioner is a protocol that is used to determine partition number for a message.</p>"},{"location":"api/aggregated_stream/#depeche_db.MessagePartitioner.get_partition","title":"<code>get_partition(message: StoredMessage[E]) -&gt; int</code>","text":"<p>Returns partition number for a message. The partition number must be a positive integer. The partition number must be deterministic for a given message.</p>"},{"location":"api/datastructures/","title":"Data structures","text":""},{"location":"api/datastructures/#depeche_db.MessageProtocol","title":"<code>depeche_db.MessageProtocol</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Message protocol is a base class for all messages that are used in the system.</p>"},{"location":"api/datastructures/#depeche_db.MessageProtocol.get_message_id","title":"<code>get_message_id()</code>","text":"<p>Returns message ID</p>"},{"location":"api/datastructures/#depeche_db.MessageProtocol.get_message_time","title":"<code>get_message_time()</code>","text":"<p>Returns message time</p>"},{"location":"api/datastructures/#depeche_db.MessagePosition","title":"<code>depeche_db.MessagePosition</code>  <code>dataclass</code>","text":"<p>Message position is a position of the message in the stream.</p> <p>Attributes:</p> Name Type Description <code>stream</code> <code>str</code> <p>Stream name</p> <code>version</code> <code>int</code> <p>Message version</p> <code>global_position</code> <code>int</code> <p>Global position</p>"},{"location":"api/datastructures/#depeche_db.StoredMessage","title":"<code>depeche_db.StoredMessage</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>Stored message is a message that is stored in the stream.</p> <p>Attributes:</p> Name Type Description <code>message_id</code> <code>UUID</code> <p>Message ID</p> <code>stream</code> <code>str</code> <p>Stream name</p> <code>version</code> <code>int</code> <p>Message version</p> <code>message</code> <code>E</code> <p>Message (<code>E</code> subtype of <code>MessageProtocol</code>)</p> <code>global_position</code> <code>int</code> <p>Global position</p>"},{"location":"api/datastructures/#depeche_db.SubscriptionMessage","title":"<code>depeche_db.SubscriptionMessage</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>Subscription message is a message that is received from the subscription.</p> <p>Attributes:</p> Name Type Description <code>partition</code> <code>int</code> <p>Partition number</p> <code>position</code> <code>int</code> <p>Position in the partition</p> <code>stored_message</code> <code>StoredMessage[E]</code> <p>Stored message (<code>E</code> subtype of <code>MessageProtocol</code>)</p>"},{"location":"api/datastructures/#depeche_db.SubscriptionState","title":"<code>depeche_db.SubscriptionState</code>  <code>dataclass</code>","text":"<p>Subscription state is a state of the subscription.</p> <p>Attributes:</p> Name Type Description <code>positions</code> <code>Dict[int, int]</code> <p>Mapping of partition number to the position in the partition</p>"},{"location":"api/event_sourcing/","title":"Event Sourcing tools","text":"<p>These classes are just given for reference. They are pretty straight-forward and you probably will implement them yourself instead of using these.</p>"},{"location":"api/event_sourcing/#depeche_db.event_sourcing.EventSourcedAggregateRoot","title":"<code>depeche_db.event_sourcing.EventSourcedAggregateRoot</code>","text":"<p>             Bases: <code>ABC</code>, <code>Generic[ID, E]</code></p> Source code in <code>depeche_db/event_sourcing/aggregate_root.py</code> <pre><code>class EventSourcedAggregateRoot(_abc.ABC, Generic[ID, E]):\n    def __init__(self):\n        self._events: list[E] = []\n        self._version = 0\n\n    @property\n    def events(self) -&gt; list[E]:\n        return list(self._events)\n\n    @_abc.abstractmethod\n    def get_id(self) -&gt; ID:\n        raise NotImplementedError\n\n    def _add_event(self, event: E) -&gt; None:\n        self._version += 1\n        self._events.append(event)\n\n    def apply(self, event: E) -&gt; None:\n        self._apply(event)\n        self._check_invariants()\n        self._add_event(event)\n\n    @_abc.abstractmethod\n    def _apply(self, event: E) -&gt; None:\n        raise NotImplementedError\n\n    def _check_invariants(self) -&gt; None:\n        pass\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, EventSourcedAggregateRoot):\n            raise NotImplementedError()\n        return self.get_id() == other.get_id() and self._version == other._version\n</code></pre>"},{"location":"api/event_sourcing/#depeche_db.event_sourcing.Repo","title":"<code>depeche_db.event_sourcing.Repo</code>","text":"<p>             Bases: <code>ABC</code>, <code>Generic[OBJ, ID]</code></p> <p>A repository is a collection of objects that can be queried and persisted.</p> <p>This is an abstract base class that defines the interface that all repositories implement.</p>"},{"location":"api/event_sourcing/#depeche_db.event_sourcing.Repo.add","title":"<code>add(entity)</code>  <code>abstractmethod</code>","text":"<p>Add a new entity to the repository.</p>"},{"location":"api/event_sourcing/#depeche_db.event_sourcing.Repo.get","title":"<code>get(id)</code>  <code>abstractmethod</code>","text":"<p>Get an entity from the repository by its ID.</p>"},{"location":"api/event_sourcing/#depeche_db.event_sourcing.Repo.save","title":"<code>save(entity, expected_version)</code>  <code>abstractmethod</code>","text":"<p>Save an existing entity to the repository.</p>"},{"location":"api/event_sourcing/#depeche_db.event_sourcing.EventStoreRepo","title":"<code>depeche_db.event_sourcing.EventStoreRepo</code>","text":"<p>             Bases: <code>Generic[E, OBJ, ID]</code>, <code>Repo[OBJ, ID]</code></p> Source code in <code>depeche_db/event_sourcing/repository.py</code> <pre><code>class EventStoreRepo(Generic[E, OBJ, ID], Repo[OBJ, ID]):\n    def __init__(\n        self,\n        event_store: MessageStore[E],\n        constructor: Callable[[], OBJ],\n        stream_prefix: str,\n    ):\n        self._event_store = event_store\n        self._constructor = constructor\n        self._stream_prefix = stream_prefix\n\n    def add(self, obj: OBJ) -&gt; MessagePosition:\n        return self.save(obj, expected_version=0)\n\n    def save(self, obj: OBJ, expected_version: int) -&gt; MessagePosition:\n        return self._event_store.synchronize(\n            stream=f\"{self._stream_prefix}-{obj.get_id()}\",\n            messages=obj.events,\n            expected_version=expected_version,\n        )\n\n    def get(self, id: ID) -&gt; OBJ:\n        with self._event_store.reader() as reader:\n            return ReadRepository[E, OBJ, ID](\n                event_store_reader=reader,\n                constructor=self._constructor,\n                stream_prefix=self._stream_prefix,\n            ).get(id)\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#depeche_db.MessageNotFound","title":"<code>depeche_db.MessageNotFound</code>","text":"<p>             Bases: <code>Exception</code></p>"},{"location":"api/exceptions/#depeche_db.OptimisticConcurrencyError","title":"<code>depeche_db.OptimisticConcurrencyError</code>","text":"<p>             Bases: <code>Exception</code></p>"},{"location":"api/executor/","title":"Executor","text":""},{"location":"api/executor/#depeche_db.Executor","title":"<code>depeche_db.Executor(db_dsn)</code>","text":"<p>Executor is a class that runs handlers on notifications.</p> <p>Typical usage:</p> <pre><code>executor = Executor(db_dsn=\"postgresql://localhost:5432/mydb\")\nexecutor.register(MyRunOnNotification())\nexecutor.run()   # this will stop on SIGTERM or SIGINT\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>db_dsn</code> <code>str</code> <p>DSN for the PostgreSQL database</p> required"},{"location":"api/executor/#depeche_db.Executor.register","title":"<code>register(handler)</code>","text":"<p>Registers a handler to be run on notifications.</p> <p>Parameters:</p> Name Type Description Default <code>handler</code> <code>RunOnNotification</code> <p>Handler to register</p> required"},{"location":"api/executor/#depeche_db.Executor.run","title":"<code>run()</code>","text":"<p>Runs the executor.</p>"},{"location":"api/executor/#depeche_db.RunOnNotification","title":"<code>depeche_db.RunOnNotification</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Run on notification is a protocol that allows objects to be run when a notification is received on a channel. Objects that implement this protocol can be registered with a Executor object.</p> Implemented by <ul> <li>SubscriptionRunner</li> <li>StreamProjector</li> </ul>"},{"location":"api/executor/#depeche_db.RunOnNotification.notification_channel","title":"<code>notification_channel: str</code>  <code>property</code>","text":"<p>Returns notification channel name.</p>"},{"location":"api/executor/#depeche_db.RunOnNotification.run","title":"<code>run()</code>","text":"<p>Runs the object. This method needs to return when a chunk of work has been done.</p>"},{"location":"api/executor/#depeche_db.RunOnNotification.stop","title":"<code>stop()</code>","text":"<p>If the object's <code>run</code> method has a loop, this method can be used to exit the loop earlier. Will be called in a separate thread.</p>"},{"location":"api/message_handlers/","title":"Message handlers","text":""},{"location":"api/message_handlers/#depeche_db.MessageHandlerRegisterProtocol","title":"<code>depeche_db.MessageHandlerRegisterProtocol</code>","text":"<p>             Bases: <code>Protocol</code>, <code>Generic[E]</code></p> <p>Message handler register protocol is used by runners to find handlers for messages.</p> Implemented by <ul> <li>MessageHandlerRegister</li> <li>MessageHandler</li> </ul>"},{"location":"api/message_handlers/#depeche_db.MessageHandlerRegisterProtocol.get_all_handlers","title":"<code>get_all_handlers()</code>","text":"<p>Returns all registered handlers.</p>"},{"location":"api/message_handlers/#depeche_db.MessageHandlerRegisterProtocol.get_handler","title":"<code>get_handler(message_type)</code>","text":"<p>Returns a handler for a given message type.</p>"},{"location":"api/message_handlers/#depeche_db.MessageHandlerRegister","title":"<code>depeche_db.MessageHandlerRegister()</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>Message handler register is a registry of message handlers.</p> <p>Typical usage:</p> <pre><code>handlers = MessageHandlerRegister()\n\n@handlers.register\ndef handle_message(message: MyMessage):\n    ...\n\n@handlers.register\ndef handle_other_message(message: StoredMessage[MyOtherMessage]):\n    ...\n</code></pre> <p>Implements: MessageHandlerRegisterProtocol</p>"},{"location":"api/message_handlers/#depeche_db.MessageHandlerRegister.register","title":"<code>register(handler)</code>","text":"<p>Registers a handler for a given message type.</p> <p>The handler must have at least one parameter. The first parameter must be of a message type. <code>E</code> being your message type, the parameter can be of type <code>E</code>, <code>SubscriptionMessage[E]</code> or <code>StoredMessage[E]</code>. When a handler is called, the message will be passed in the requested type.</p> <p>Multiple handlers can be registered for non-overlapping types of messages. Overlaps will cause a <code>ValueError</code>.</p> <p>Parameters:</p> Name Type Description Default <code>handler</code> <code>H</code> <p>A handler function.</p> required <p>Returns:</p> Type Description <code>H</code> <p>The unaltered handler function.</p>"},{"location":"api/message_handlers/#depeche_db.MessageHandlerRegister.register_manual","title":"<code>register_manual(handler, handled_type, requires_middleware=False)</code>","text":"<p>Registers a handler for a given message type.</p> <p>The handler must have at least one parameter. The first parameter must be of a message type.</p> <p>Same overlap rules apply to this method as to the <code>register</code> method.</p> <p>If the handler takes more than one parameter, you must set the <code>requires_middleware</code> parameter to <code>True</code>!</p> <p>Parameters:</p> Name Type Description Default <code>handler</code> <code>H</code> <p>A handler function.</p> required <code>handled_type</code> <code>Type</code> <p>The type of message to handle.</p> required <code>requires_middleware</code> <code>bool</code> <p>Whether the handler requires middleware.</p> <code>False</code>"},{"location":"api/message_handlers/#depeche_db.MessageHandler","title":"<code>depeche_db.MessageHandler()</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>Message handler is a base class for message handlers.</p> <p>This is basically a class-based version of the <code>MessageHandlerRegister</code>.</p> <p>Typical usage (equivalent to the example in <code>MessageHandlerRegister</code>):</p> <pre><code>class MyMessageHandler(MessageHandler):\n    @MessageHandler.register\n    def handle_message(self, message: MyMessage):\n        ...\n\n    @MessageHandler.register\n    def handle_other_message(self, message: StoredMessage[MyOtherMessage]):\n        ...\n</code></pre> <p>Implements: MessageHandlerRegisterProtocol</p>"},{"location":"api/message_store/","title":"MessageStore","text":""},{"location":"api/message_store/#depeche_db.MessageStore","title":"<code>depeche_db.MessageStore(name, engine, serializer)</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>Message store.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A valid python identifier which is used as a prefix for the database objects that are created.</p> required <code>engine</code> <code>Engine</code> <p>A SQLAlchemy engine.</p> required <code>serializer</code> <code>MessageSerializer</code> <p>A serializer for the messages.</p> required <p>Attributes:</p> Name Type Description <code>aggregated_stream</code> <code>AggregatedStreamFactory</code> <p>A factory for aggregated streams.</p>"},{"location":"api/message_store/#depeche_db.MessageStore.read","title":"<code>read(stream)</code>","text":"<p>Read all messages from a stream.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>str</code> <p>The name of the stream.</p> required <p>Returns:</p> Type Description <code>Iterator[StoredMessage[E]]</code> <p>Iterator[StoredMessage]: An iterator over the messages.</p>"},{"location":"api/message_store/#depeche_db.MessageStore.reader","title":"<code>reader(conn=None)</code>","text":"<p>Get a reader for the store.</p> <p>You can give a connection to use for the read as <code>conn</code>. If you don't give a connection, a new connection will be used (and discarded after the reader context has been left).</p> <p>Example usage:</p> <pre><code>with store.reader() as reader:\n    message = reader.get_message_id(some_id)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>Optional[SAConnection]</code> <p>A database connection.</p> <code>None</code> <p>Yields:</p> Name Type Description <code>MessageStoreReader</code> <code>MessageStoreReader[E]</code> <p>A reader for the store.</p>"},{"location":"api/message_store/#depeche_db.MessageStore.synchronize","title":"<code>synchronize(stream, expected_version, messages, conn=None)</code>","text":"<p>Synchronize a stream with a sequence of messages.</p> <p>Given a stream and a sequence of messages, this method will write the messages to the stream that are not already in it.</p> <p>Optimistic concurrency control must used to ensure that the stream is not modified by another process between reading the last message and writing the new message. You have to give <code>expected_version</code>. If the stream has been modified, a <code>OptimisticConcurrencyError</code> will be raised.</p> <p>You can give a connection to use for the write as <code>conn</code>. If you don't give a connection, a new connection will be created and the write will be committed. You can use this to write messages and other data in a single transaction. Therefore, if you give a connection, you have to commit the transaction yourself.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>str</code> <p>The name of the stream to which the message should be written.</p> required <code>expected_version</code> <code>int</code> <p>The expected version of the stream.</p> required <code>messages</code> <code>Sequence[MessageProtocol]</code> <p>The messages that should be in the stream after the synchronization.</p> required <code>conn</code> <code>Optional[SAConnection]</code> <p>A database connection.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>MessagePosition</code> <code>MessagePosition</code> <p>The position of the last message in the stream.</p>"},{"location":"api/message_store/#depeche_db.MessageStore.write","title":"<code>write(stream, message, expected_version=None, conn=None)</code>","text":"<p>Write a message to the store.</p> <p>Optimistic concurrency control can used to ensure that the stream is not modified by another process between reading the last message and writing the new message. You have to give <code>expected_version</code> to use it. If the stream has been modified, a <code>OptimisticConcurrencyError</code> will be raised.</p> <p>You can give a connection to use for the write as <code>conn</code>. If you don't give a connection, a new connection will be created and the write will be committed. You can use this to write messages and other data in a single transaction. Therefore, if you give a connection, you have to commit the transaction yourself.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>str</code> <p>The name of the stream to which the message should be written.</p> required <code>message</code> <code>MessageProtocol</code> <p>The message to write.</p> required <code>expected_version</code> <code>Optional[int]</code> <p>The expected version of the stream.</p> <code>None</code> <code>conn</code> <code>Optional[SAConnection]</code> <p>A database connection.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>MessagePosition</code> <code>MessagePosition</code> <p>The position of the last message in the stream.</p>"},{"location":"api/message_store/#depeche_db.MessageStoreReader","title":"<code>depeche_db.MessageStoreReader(conn, storage, serializer)</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>Message store reader.</p>"},{"location":"api/message_store/#depeche_db.MessageStoreReader.get_message_by_id","title":"<code>get_message_by_id(message_id)</code>","text":"<p>Returns a message by ID.</p> <p>Parameters:</p> Name Type Description Default <code>message_id</code> <code>UUID</code> <p>Message ID.</p> required"},{"location":"api/message_store/#depeche_db.MessageStoreReader.get_messages_by_ids","title":"<code>get_messages_by_ids(message_ids)</code>","text":"<p>Returns multiple messages by IDs.</p> <p>Parameters:</p> Name Type Description Default <code>message_ids</code> <code>Sequence[UUID]</code> <p>Message IDs.</p> required"},{"location":"api/message_store/#depeche_db.MessageStoreReader.read","title":"<code>read(stream)</code>","text":"<p>Returns all messages from a stream.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>str</code> <p>Stream name</p> required"},{"location":"api/message_store/#depeche_db.MessageStoreReader.read_wildcard","title":"<code>read_wildcard(stream_wildcard)</code>","text":"<p>Returns all messages from streams that match the wildcard.</p> <p>Use like syntax to match multiple streams:</p> <ul> <li><code>stream-%</code> - match all streams that start with <code>stream-</code></li> <li><code>%</code> - match all streams</li> <li><code>%-%</code> - match all streams that contain <code>-</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>stream_wildcard</code> <code>str</code> <p>Stream name wildcard</p> required"},{"location":"api/message_store/#depeche_db.MessageSerializer","title":"<code>depeche_db.MessageSerializer</code>","text":"<p>             Bases: <code>Protocol</code>, <code>Generic[M]</code></p> <p>Message serializer is a protocol that is used to serialize and deserialize messages.</p> <p>The following must be true for any serializer:</p> <ul> <li><code>deserialize(serialize(message)) == message</code></li> <li><code>type(deserialize(serialize(message))) is type(message)</code></li> <li><code>serialize(deserialize(data)) == data</code></li> </ul>"},{"location":"api/message_store/#depeche_db.MessageSerializer.deserialize","title":"<code>deserialize(message: dict) -&gt; M</code>","text":"<p>Deserializes message from a dictionary.</p>"},{"location":"api/message_store/#depeche_db.MessageSerializer.serialize","title":"<code>serialize(message: M) -&gt; dict</code>","text":"<p>Serializes message to a dictionary. The dictionary must be JSON serializable.</p>"},{"location":"api/subscription/","title":"Subscription","text":""},{"location":"api/subscription/#depeche_db.SubscriptionFactory","title":"<code>depeche_db.SubscriptionFactory(stream)</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>This factory is accessible on the aggregated stream:</p> <pre><code>stream : AggregatedStream = ...\nsubscription = stream.subscription(\n    name=\"subscription_name\",\n    handlers=...,\n    call_middleware=...,\n    error_handler=...,\n    state_provider=...,\n    lock_provider=...,\n)\n</code></pre>"},{"location":"api/subscription/#depeche_db.SubscriptionFactory.__call__","title":"<code>__call__(name, handlers=None, batch_size=None, call_middleware=None, error_handler=None, state_provider=None, lock_provider=None, start_point=None)</code>","text":"<p>Create a subscription.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the subscription, needs to be a valid python identifier</p> required <code>handlers</code> <code>Optional[MessageHandlerRegisterProtocol[E]]</code> <p>Handlers to be called when a message is received, defaults to an empty register</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>Number of messages to read at once, defaults to 10, read more here</p> <code>None</code> <code>call_middleware</code> <code>Optional[CallMiddleware]</code> <p>A middleware to customize the call to the handlers</p> <code>None</code> <code>error_handler</code> <code>Optional[SubscriptionErrorHandler]</code> <p>A handler for errors raised by the handlers, defaults to handler that will exit the subscription</p> <code>None</code> <code>state_provider</code> <code>Optional[SubscriptionStateProvider]</code> <p>Provider for the subscription state, defaults to a PostgreSQL provider</p> <code>None</code> <code>lock_provider</code> <code>Optional[LockProvider]</code> <p>Provider for the locks, defaults to a PostgreSQL provider</p> <code>None</code> <code>start_point</code> <code>Optional[SubscriptionStartPoint]</code> <p>The start point for the subscription, defaults to beginning of the stream</p> <code>None</code>"},{"location":"api/subscription/#depeche_db.Subscription","title":"<code>depeche_db.Subscription(name, stream, message_handler, batch_size=None, state_provider=None, lock_provider=None, start_point=None)</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>A subscription is a way to read messages from an aggregated stream.</p> <p>Read more about the subscription in the concepts section.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the subscription, needs to be a valid python identifier</p> required <code>stream</code> <code>AggregatedStream[E]</code> <p>Stream to read from</p> required <code>message_handler</code> <code>SubscriptionMessageHandler[E]</code> <p>Handler for the messages</p> required <code>batch_size</code> <code>Optional[int]</code> <p>Number of messages to read at once, defaults to 10, read more here</p> <code>None</code> <code>state_provider</code> <code>Optional[SubscriptionStateProvider]</code> <p>Provider for the subscription state, defaults to a PostgreSQL provider</p> <code>None</code> <code>lock_provider</code> <code>Optional[LockProvider]</code> <p>Provider for the locks, defaults to a PostgreSQL provider</p> <code>None</code> <code>start_point</code> <code>Optional[SubscriptionStartPoint]</code> <p>The start point for the subscription, defaults to beginning of the stream</p> <code>None</code>"},{"location":"api/subscription/#depeche_db.CallMiddleware","title":"<code>depeche_db.CallMiddleware</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>Call middleware is a protocol that is used to wrap a call to a handler.</p> <p>Typical implementation:</p> <pre><code>class MyCallMiddleware(CallMiddleware):\n    def __init__(self, some_dependency):\n        self.some_dependency = some_dependency\n\n    def call(self, handler, message):\n        # or use a DI container here\n        handler(message, some_dependency=self.some_dependency)\n</code></pre>"},{"location":"api/subscription/#depeche_db.CallMiddleware.call","title":"<code>call(handler, message)</code>","text":"<p>Calls a handler with a given message.</p> <p>The type of the message depends on the type annotation of the handler function. See MessageHandlerRegister for more details.</p> <p>Parameters:</p> Name Type Description Default <code>handler</code> <code>Callable</code> <p>Handler</p> required <code>message</code> <code>Union[SubscriptionMessage[E], StoredMessage[E], E]</code> <p>Message to be passed to the handler</p> required"},{"location":"api/subscription/#depeche_db.SubscriptionErrorHandler","title":"<code>depeche_db.SubscriptionErrorHandler</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>Subscription error handler is a protocol that is used to handle errors that occur.</p>"},{"location":"api/subscription/#depeche_db.SubscriptionErrorHandler.handle_error","title":"<code>handle_error(error: Exception, message: SubscriptionMessage[E]) -&gt; ErrorAction</code>","text":"<p>Handles an error that occurred during message processing.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>Error</p> required <code>message</code> <code>SubscriptionMessage[E]</code> <p>Message that was being processed when the error occurred</p> required <p>Returns:</p> Type Description <code>ErrorAction</code> <p>Action to be taken</p>"},{"location":"api/subscription/#depeche_db.ErrorAction","title":"<code>depeche_db.ErrorAction</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Error action is an action that is taken when an error occurs during message processing.</p> <p>Attributes:</p> Name Type Description <code>IGNORE</code> <p>Ignore the error and continue processing.</p> <code>EXIT</code> <p>Exit processing.</p>"},{"location":"api/subscription/#depeche_db.ExitSubscriptionErrorHandler","title":"<code>depeche_db.ExitSubscriptionErrorHandler</code>","text":"<p>             Bases: <code>SubscriptionErrorHandler</code></p> <p>Exit the subscription on error</p>"},{"location":"api/subscription/#depeche_db.LogAndIgnoreSubscriptionErrorHandler","title":"<code>depeche_db.LogAndIgnoreSubscriptionErrorHandler(subscription_name)</code>","text":"<p>             Bases: <code>SubscriptionErrorHandler</code></p> <p>Log the error and ignore the message</p>"},{"location":"api/subscription/#depeche_db.SubscriptionMessageHandler","title":"<code>depeche_db.SubscriptionMessageHandler(handler_register, error_handler=None, call_middleware=None)</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>Handles messages</p> <p>Parameters:</p> Name Type Description Default <code>handler_register</code> <code>MessageHandlerRegisterProtocol[E]</code> <p>The handler register to use</p> required <code>error_handler</code> <code>Optional[SubscriptionErrorHandler]</code> <p>A handler for errors raised by the handlers, defaults to handler that will exit the subscription</p> <code>None</code> <code>call_middleware</code> <code>Optional[CallMiddleware]</code> <p>The middleware to call before calling the handler</p> <code>None</code>"},{"location":"api/subscription/#depeche_db.SubscriptionRunner","title":"<code>depeche_db.SubscriptionRunner(subscription, message_handler, batch_size=None)</code>","text":"<p>             Bases: <code>Generic[E]</code></p> <p>Handles messages from a subscription using a handler</p> <p>The <code>batch_size</code> argument controls how many messages to handle in each batch. If not provided, the default is 10. A larger batch size will result less round trips to the database, but will also make it more likely that messages from different partitions will be processed out of the order defined by their <code>global_position</code> on the message store.</p> <p>A batch size of 1 will ensure that messages are processed in order regarding to their <code>global_position</code>. Messages in the same partition will always be processed in order.</p> <p>Implements: RunOnNotification</p> <p>Parameters:</p> Name Type Description Default <code>subscription</code> <code>Subscription[E]</code> <p>The subscription to handle</p> required <code>message_handler</code> <code>SubscriptionMessageHandler</code> <p>The handler to use</p> required <code>batch_size</code> <code>Optional[int]</code> <p>The number of messages to handle in each batch, defaults to 10</p> <code>None</code>"},{"location":"api/subscription/#depeche_db.LockProvider","title":"<code>depeche_db.LockProvider</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Lock provider is a protocol that is used to lock and unlock resources.</p>"},{"location":"api/subscription/#depeche_db.LockProvider.lock","title":"<code>lock(name: str) -&gt; bool</code>","text":"<p>Locks resource with a given name. Returns <code>True</code> if the resource was locked. This method must not block!</p>"},{"location":"api/subscription/#depeche_db.LockProvider.unlock","title":"<code>unlock(name: str)</code>","text":"<p>Unlocks resource with a given name.</p>"},{"location":"api/subscription/#depeche_db.SubscriptionStateProvider","title":"<code>depeche_db.SubscriptionStateProvider</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Subscription state provider is a protocol that is used to store and read subscription state.</p>"},{"location":"api/subscription/#depeche_db.SubscriptionStateProvider.initialize","title":"<code>initialize(subscription_name: str)</code>","text":"<p>Marks subscription state as initialized.</p>"},{"location":"api/subscription/#depeche_db.SubscriptionStateProvider.initialized","title":"<code>initialized(subscription_name: str) -&gt; bool</code>","text":"<p>Returns <code>True</code> if the subscription state was already initialized.</p>"},{"location":"api/subscription/#depeche_db.SubscriptionStateProvider.read","title":"<code>read(subscription_name: str) -&gt; SubscriptionState</code>","text":"<p>Reads subscription state.</p> <p>Returns:</p> Type Description <code>SubscriptionState</code> <p>Subscription state</p>"},{"location":"api/subscription/#depeche_db.SubscriptionStateProvider.store","title":"<code>store(subscription_name: str, partition: int, position: int)</code>","text":"<p>Stores subscription state for a given partition.</p>"},{"location":"api/subscription/#depeche_db.SubscriptionStartPoint","title":"<code>depeche_db.SubscriptionStartPoint</code>","text":"<p>Defines the start point of a subscription.</p>"},{"location":"api/subscription/#depeche_db.SubscriptionStartPoint.init_state","title":"<code>init_state(subscription_name, stream, state_provider)</code>","text":"<p>Initializes subscription state (if not yet initialized).</p>"},{"location":"api/subscription/#depeche_db.StartAtNextMessage","title":"<code>depeche_db.StartAtNextMessage</code>","text":"<p>             Bases: <code>SubscriptionStartPoint</code></p> <p>Starts consuming messages from the next message in the stream.</p>"},{"location":"api/subscription/#depeche_db.StartAtPointInTime","title":"<code>depeche_db.StartAtPointInTime(point_in_time)</code>","text":"<p>             Bases: <code>SubscriptionStartPoint</code></p> <p>Starts consuming messages from a point in time.</p> <p>Parameters:</p> Name Type Description Default <code>point_in_time</code> <code>datetime</code> <p>The point in time to start consuming messages from. The point in time must be timezone aware.</p> required"},{"location":"concepts/data-model/","title":"Data model","text":""},{"location":"concepts/data-model/#messages","title":"Messages","text":"<p>The smallest unit of storage is a message. The content of messages is transparent to Depeche DB. Every message needs two basic properties so that it can be stored and processed: An <code>ID</code> of type UUID and a timestamp.</p>"},{"location":"concepts/data-model/#streams","title":"Streams","text":"<p>A stream has a name and contains messages. The message store can contain as many streams as you need.</p> <p>flowchart TB     subgraph stream-1     msg11 --&gt; msg12 --&gt; msg13     end  flowchart TB     subgraph stream-2     msg21 --&gt; msg22     end </p> <p>Messages in a stream are ordered. Their position within the stream is called <code>version</code>. The <code>version</code>s in a stream do not have gaps and are unique.</p> <p>Usually, a stream will be relatively small because it only contains messages concerning a single domain object.</p>"},{"location":"concepts/data-model/#message-store","title":"Message store","text":"<p>The message store contains multiple streams. Messages are assigned a global position when they are added to a stream. This global position is unique but there can be gaps (when transactions are rolled back). For messages <code>a</code> and <code>b</code> within the same stream, it is guaranteed that if <code>a.version &lt; b.version</code> then <code>a.global_position &lt; b.global_position</code> and vice versa.</p>"},{"location":"concepts/data-model/#aggregated-streams","title":"Aggregated streams","text":"<p>Because of the high granularity of the streams, it is not pratical to subscribe to them directly. That is why there is the notion of an aggregated stream which aggregates the messages from several streams. The selection of the origin streams is done by match expressions, e.g. the expression <code>foo-%</code> will match streams like <code>foo-1</code>, <code>foo-123</code> etc. An origin stream can be part of multiple aggregated streams.</p> <p>Aggregated streams consist of partitions. Messages from the origin streams can be assigned to partitions by a user defined function. When a message is added to a partition within an aggregated stream, it is not copied but linked to. Thus, aggregated streams are rather light-weight on the storage size.</p> <p>The partitions is what gives us the possibility of concurrency while processing messages. You have to be careful when selecting a partition key or function. Messages which must be processed in the order in which they occurred in need to end up in the same partition.</p> <p>Given these streams: flowchart TB     subgraph stream-1     msg11 --&gt; msg12 --&gt; msg13     style msg11 fill:#feaaaa     style msg12 fill:#feaaaa     style msg13 fill:#feaaaa     end     subgraph stream-2     msg21 --&gt; msg22     style msg21 fill:#aafeaa     style msg22 fill:#aafeaa     end     subgraph stream-3     msg31 --&gt; msg32     style msg31 fill:#aaaafe     style msg32 fill:#aaaafe     end </p> <p>The partitions of an example aggregated stream could look like this:</p> <p>flowchart TB     subgraph partition-1       p11 --&gt; p12 --&gt; p31 --&gt; p13 --&gt; p32       style p11 fill:#feaaaa       style p12 fill:#feaaaa       style p13 fill:#feaaaa       style p31 fill:#aaaafe       style p32 fill:#aaaafe     end  flowchart TB     subgraph partition-2       p21 --&gt; p22       style p21 fill:#aafeaa       style p22 fill:#aafeaa     end </p> <p>Messages are assigned a unique &amp; gapless <code>position</code> within their partition. The order of messages within a partition is based on their global position in the message store. This cannot be guaranteed though, because transactions writing to the messages store might be committed \"out of order\". The messages will be out of order concerning their global position, if the aggregated stream is updated between those commits.</p> <p>Please note that this example partitioning (which used the stream name as the partition key) is only one possibility. Users can define how messages are partitioned. Furthermore, you can create multiple aggregated streams which contain the same or overlapping sets of messages with different partitioning strategies. This allows you to tailor the aggregated streams - more specifically their ordering and possible parallelism - to the needs of different consumers.</p>"},{"location":"concepts/subscriptions/","title":"Subscriptions","text":"<p>Given an aggregated stream, we can create a number of subscriptions on it which are discriminated by a name. A subscription works roughly like this:</p> <ol> <li>Wait for a trigger</li> <li>Get the current state (pairs of partition number &amp; last processed position)</li> <li>For each partition that has messages after the known position (ordered by the    message time of the oldest unprocessed message)<ol> <li>Try to acquire a lock (subscription group name, partition)</li> <li>Re-validate the state (a parallel process might have advanced it already)</li> <li>Process the message(s) (depending on the batch size)</li> <li>Update the state with the new position</li> <li>Release the lock</li> </ol> </li> </ol> <p>This algorithm allows us to process messages concurrently while still honoring the ordering guarantees within a partition because only one instance can process messages from any given partition at any time. It can be described as an instance of the competing consumers pattern.</p> <p>The ordering done in step 3 is not necessary to keep any of the guarantees but helps to keep up with the expectation that messages in the system are processed roughly in the order given by their message time. It also helps with fairness, because it will favor processing older messages first.</p> <p>The order of steps 3c and 3d makes this a \"at least once\" delivery system, because the message is processed before the new position is recorded.</p>"},{"location":"concepts/subscriptions/#start-point","title":"Start point","text":"<p>When a subscription run for the first time, it decide where it should start consuming messages. Three options exist:</p> <ul> <li>Beginning of the aggregated stream (default)</li> <li>Next message: Start at the first message that is appended to the stream   after the subscription started</li> <li>Point in time: Start at messages with a message time greater or equal a given   datetime.</li> </ul>"},{"location":"concepts/subscriptions/#services-required-by-subscriptions","title":"Services required by subscriptions","text":"<p>A subscription requires two services provided to it:</p> <ol> <li>Subscription state storage</li> <li>(Distributed) Locks</li> </ol> <p>Both of the services have a default implementation but it may be a good idea to check if you should customize/replace it to your needs. The interfaces of both services are pretty simple.</p>"},{"location":"concepts/subscriptions/#state","title":"State","text":"<p>The best location for the subscription state is - especially in the context of transactionally safe storage - close to the state that is altered in the course of the application code handling messages.</p> <p>If you can manage to get the subscription state updates within the same transaction as the application state, you essentially implemented a \"exactly-once\" message processing mechanism.</p>"},{"location":"concepts/subscriptions/#locks","title":"Locks","text":"<p>The default locking provider uses again PostgreSQL, namely advisory locks through the <code>pals</code> library. If for some reason your system is not actually distributed to multiple machines, you could replace if with a simpler &amp; faster locking system. Or maybe you already have a distributed locking infrastructure, then it might be worth a look to use it here too.</p>"},{"location":"examples/bank-eventsourced/","title":"Example: Bank","text":"<p>A bank account is classic example when event sourcing is shown.</p>"},{"location":"examples/bank-eventsourced/#repo","title":"Repo","text":"<p>https://github.com/depeche-py/example-bank</p>"},{"location":"examples/bank-eventsourced/#walk-through","title":"Walk through","text":""},{"location":"examples/bank-eventsourced/#bankapi","title":"<code>bank/api</code>","text":"<p>A simple REST API using FastAPI</p>"},{"location":"examples/bank-eventsourced/#bankmessagespy","title":"<code>bank/messages.py</code>","text":"<p>Defines a base class for all messages in the system as well as a couple of command and event messages, e.g.</p> <pre><code>class CreateAccountCommand(Message):\n    account_id: _uuid.UUID | None = None\n    account_number: str\n\n\nclass AccountCreatedEvent(Message):\n    account_id: _uuid.UUID\n    account_number: str\n</code></pre>"},{"location":"examples/bank-eventsourced/#bankdomainpy","title":"<code>bank/domain.py</code>","text":"<p>The definition of the Domain objects</p>"},{"location":"examples/bank-eventsourced/#account","title":"<code>Account</code>","text":"<p>Account has three operations (create, withdraw, deposit) and tracks the resulting balance.</p> <p>The operations are implemented in such a way that they just append &amp; apply a new event. The <code>_apply</code> method is called by the inherited <code>apply</code> method. It is the projection that will be applied for each event whenever the state needs to be (re)constructed from them.</p> <pre><code>class Account(_es.EventSourcedAggregateRoot[_uuid.UUID, _messages.AccountEvent]):\n    id: _uuid.UUID\n    number: str\n    balance: int\n\n    def _apply(self, event: _messages.AccountEvent) -&gt; None:\n        if isinstance(event, _messages.AccountCreatedEvent):\n            self.id = event.account_id\n            self.number = event.account_number\n            self.balance = 0\n        # over event types...\n\n    @classmethod\n    def create(cls, account_id: _uuid.UUID, number: str) -&gt; \"Account\":\n        account = cls()\n        account.apply(\n            _messages.AccountCreatedEvent(account_id=account_id, account_number=number)\n        )\n        return account\n\n    #...\n</code></pre>"},{"location":"examples/bank-eventsourced/#transfer","title":"<code>Transfer</code>","text":"<p>Is used to keep the state for transfer from one to another <code>Account</code>. Together with the event handlers it can be seen as sort of a process manager.</p>"},{"location":"examples/bank-eventsourced/#bankhandlers","title":"<code>bank/handlers</code>","text":""},{"location":"examples/bank-eventsourced/#commands","title":"<code>commands</code>","text":"<p>Defines a class that handles command messages. It uses repositories to load/save the event-sourced domain objects. The command handler is used both from the REST API and from a subscription on a stream of command messages.</p>"},{"location":"examples/bank-eventsourced/#events","title":"<code>events</code>","text":"<p>Defines classes that handle events. The event handlers are used by subscriptions to event streams.</p>"},{"location":"examples/bank-eventsourced/#queries","title":"<code>queries</code>","text":"<p>Defines a class that is used by the REST API to get domain objects.</p>"},{"location":"examples/bank-eventsourced/#bankinfra","title":"<code>bank/infra</code>","text":"<p>In this file, all the infrastructure is defined. Most notably, we define:</p> <ul> <li>a dependency injection container (using the <code>explicit-di</code> library which is used to simplify the creation of &amp; calls to the handler objects.</li> <li>aggregate streams and subscriptions</li> </ul>"},{"location":"generated/","title":"Index","text":""},{"location":"generated/#depeche-db","title":"Depeche DB","text":"<p>A library for building event-based systems on top of PostgreSQL</p> <p> </p> <p>Documentation: https://depeche-py.github.io/depeche-db/</p> <p>Source code: https://github.com/depeche-py/depeche-db</p> <p>Depeche DB is modern Python library for building event-based systems</p> <p>Key features:</p> <ul> <li>Message store with optimistic concurrency control &amp; strong ordering guarantees</li> <li>Subscriptions with \"at least once\" semantics</li> <li>No database polling</li> </ul>"},{"location":"generated/#requirements","title":"Requirements","text":"<p>Python 3.9+</p> <p>SQLAlchemy 1.4 or 2+</p> <p>PostgreSQL 12+</p>"},{"location":"generated/#installation","title":"Installation","text":"<pre><code>pip install depeche-db\n# OR\npoetry add depeche-db\n</code></pre>"},{"location":"generated/#example","title":"Example","text":"<pre><code>%EXAMPLE%\n</code></pre>"},{"location":"generated/#contribute","title":"Contribute","text":"<p>Contributions in the form of issues, questions, feedback and pull requests are welcome. Before investing a lot of time, let me know what you are up to so we can see if your contribution fits the vision of the project.</p>"},{"location":"generated/output/getting-started-aggregated-stream/","title":"Aggregated stream","text":"<p>We will use the same message store as in the previous chapter here, but we will create a new set of streams within it:</p> <pre><code>import random\n\nfor _ in range(20):\n    n = random.randint(0, 200)\n    stream = f\"aggregate-me-{n % 5}\"\n    message_store.write(stream=stream, message=EventA(num=n))\n</code></pre> <p>For our aggregated stream, we need to prepare a partition function (or rather class).</p> <pre><code>from depeche_db import StoredMessage\n\n\nclass NumMessagePartitioner:\n    def get_partition(self, message: StoredMessage[EventA | EventB]) -&gt; int:\n        if isinstance(message.message, EventA):\n            return message.message.num % 3\n        return 0\n</code></pre> <p>Now we can put together the aggregated stream.</p> <pre><code>aggregated_stream = message_store.aggregated_stream(\n    name=\"example_docs_aggregate_me2\",\n    partitioner=NumMessagePartitioner(),\n    stream_wildcards=[\"aggregate-me-%\"],\n)\naggregated_stream.projector.update_full()\n</code></pre> <p>Whenever we call <code>update_full</code>, all new messages in the origin streams will be appended to the relevant partition of the aggregated stream in the right order. We will not have to call this manually though. We can use the <code>Executor</code> to do it for us.</p> <p>We can read from the aggregated stream directly:</p> <pre><code>print(next(aggregated_stream.read(partition=2)))\n#  AggregatedStreamMessage(\n#      partition=2,\n#      position=0,\n#      message_id=UUID(\"1f804185-e63d-462e-b996-d6f16e5ff8af\")\n#  )\n</code></pre> <p>The <code>AggregatedStreamMessage</code> object contains minimal metadata about the message in the context of the aggregated stream. It does not contain the original message though. To get that, we need to use the message store reader.</p> <p>Usually though we will not read the aggregated stream directly, but rather use a subscription to consume it. We will get to that in the next chapter.</p>"},{"location":"generated/output/getting-started-subscription/","title":"Subscription","text":"<p>Given the aggregated stream from the previous chapter, we can put together a subscription.</p> <pre><code>subscription = aggregated_stream.subscription(\n    name=\"sub_example_docs_aggregate_me\",\n)\n</code></pre> <p>You can read from a subscription directly. Whenever <code>get_next_messages</code> emits a message, it will update the position of the subscription, so that the next call will return the next message.</p> <p>The emitted message is wrapped in a <code>SubscriptionMessage</code> object which contains the metadata about the message in the context of the subscription/aggregated stream.</p> <pre><code>for message in subscription.get_next_messages(count=1):\n    print(message)\n#  SubscriptionMessage(\n#      partition=2,\n#      position=0,\n#      stored_message=StoredMessage(\n#          message_id=UUID(\"1f804185-e63d-462e-b996-d6f16e5ff8af\"),\n#          stream=\"aggregate-me-1\",\n#          version=1,\n#          message=EventA(\n#              event_id=UUID(\"1f804185-e63d-462e-b996-d6f16e5ff8af\"),\n#              happened_at=datetime.datetime(2023, 10, 5, 20, 3, 26, 658725),\n#              num=176,\n#          ),\n#          global_position=4,\n#      ),\n#  )\n</code></pre> <p>Reading from a subscription directly is not the most common use case though. In order to continously handle messages on a subscription we create a <code>MessageHandlerRegister</code> and pass this in when we create the subscription.</p> <p>On the <code>MessageHandlerRegister</code> we register a handler for the message type(s) we are interested in. You can register multiple handlers for different message types but the handled message types must not overlap. Given your message type <code>E</code>, you can request <code>SubscriptionMessage[E]</code>, <code>StoredMessage[E]</code> or <code>E</code> as the type of the argument to the handler by using type hints.</p> <pre><code>from depeche_db import SubscriptionMessage, MessageHandlerRegister\n\nhandlers = MessageHandlerRegister[EventA | EventB]()\n\n\n@handlers.register\ndef handle_event_a(msg: SubscriptionMessage[EventA]):\n    real_message = msg.stored_message.message\n    print(f\"num={real_message.num} (partition {msg.partition} at {msg.position})\")\n</code></pre> <p>Now we can create a new subscription with these handlers.</p> <pre><code>subscription = aggregated_stream.subscription(\n    name=\"sub_example_docs_aggregate_me_with_handlers\",\n    handlers=handlers,\n)\n</code></pre> <p>Running <code>run_once</code> will read the unprocessed messages from the subscription and call the registered handlers (if any).</p> <pre><code>subscription.runner.run_once()\n#  num=111 (partition 0 at 0)\n#  num=199 (partition 1 at 0)\n#  num=166 (partition 1 at 1)\n#  num=0 (partition 0 at 1)\n#  num=152 (partition 2 at 0)\n#  num=172 (partition 1 at 2)\n#  num=12 (partition 0 at 2)\n#  ...\n</code></pre> <p>Running <code>run_once</code> will read the unprocessed messages from the subscription and call the registered handlers (if any).</p> <p>In a real application, we would not call <code>run_once</code> directly, but we would use the <code>Executor</code> to do it for us.</p> <p>A subscription by default starts at the beginning of the stream. If we want to change this behaviour, we can pass in a <code>SubscriptionStartPosition</code> object when we create the subscription. This object can be one of the following: <pre><code>from datetime import timezone\nfrom depeche_db import StartAtNextMessage, StartAtPointInTime\n\nsubscription_next = aggregated_stream.subscription(\n    name=\"sub_example_docs_aggregate_me_next\", start_point=StartAtNextMessage()\n)\n\nsubscription_point_in_time = aggregated_stream.subscription(\n    name=\"sub_example_docs_aggregate_me_next\",\n    start_point=StartAtPointInTime(\n        datetime(2023, 10, 5, 14, 0, 0, 0, tzinfo=timezone.utc)\n    ),\n)\n</code></pre></p>"},{"location":"generated/output/getting-started-write-read/","title":"Writing &amp; reading messages","text":"<p>First, create a SQLAlchemy engine with your database connection: <pre><code>from sqlalchemy import create_engine\n\nDB_DSN = \"postgresql://depeche:depeche@localhost:4888/depeche_demo\"\ndb_engine = create_engine(DB_DSN)\n</code></pre></p> <p>Then we define our message types using pydantic. Using pydantic is optional, but it makes serialization straightforward. <pre><code>from datetime import datetime\nfrom uuid import UUID, uuid4\n\nimport pydantic\n\n\nclass MyEvent(pydantic.BaseModel):\n    event_id: UUID = pydantic.Field(default_factory=uuid4)\n    happened_at: datetime = pydantic.Field(default_factory=datetime.utcnow)\n\n    def get_message_id(self) -&gt; UUID:\n        return self.event_id\n\n    def get_message_time(self) -&gt; datetime:\n        return self.happened_at\n\n\nclass EventA(MyEvent):\n    num: int\n\n\nclass EventB(MyEvent):\n    text: str\n</code></pre></p> <p>Now we are ready to create our message store. This will create a new table <code>example_basic_messages</code> when it is called the first time. <pre><code>from depeche_db import MessageStore\nfrom depeche_db.tools import PydanticMessageSerializer\n\nmessage_store = MessageStore[EventA | EventB](\n    name=\"example_docs2\",\n    engine=db_engine,\n    serializer=PydanticMessageSerializer(EventA | EventB),\n)\n</code></pre></p> <p>Now we write an event to the stream</p> <pre><code>stream = f\"stream-{uuid4()}\"\n\nresult = message_store.write(stream=stream, message=EventA(num=42))\nprint(result)\n#  MessagePosition(stream='stream-&lt;uuid&gt;', version=1, global_position=1)\n</code></pre> <p>Here is how we can read the messages: <pre><code>print(next(message_store.read(stream)))\n#  StoredMessage(\n#    message_id=UUID('&lt;uuid&gt;'),\n#    stream='stream-&lt;uuid&gt;',\n#    version=1,\n#    message=EventA(\n#      event_id=UUID('&lt;uuid&gt;'),\n#      num=42,\n#      happened_at=datetime.datetime(...)\n#    ),\n#    global_position=1\n#  )\n</code></pre></p> <p>Please note that when reading, the original message is wrapped in a <code>StoredMessage</code> object, which contains the metadata about the message.</p> <p>When we write, we can pass an <code>expected_version</code> parameter, which gives us optimistic concurrency control. <pre><code># this fails because the expected version is 0, but the stream already has a message\nmessage_store.write(stream=stream, message=EventA(num=23), expected_version=0)\n# this is fine, because we expect the right version\nmessage_store.write(stream=stream, message=EventA(num=23), expected_version=1)\n</code></pre></p>"},{"location":"getting-started/executor/","title":"Executor","text":"<p>Executing the regular jobs of</p> <ul> <li>updating aggregated streams</li> <li>running handlers on subscriptions</li> </ul> <p>by hand is cumbersome. <code>StreamProjector</code> and <code>SubscriptionHandler</code> both implement the <code>RunOnNotification</code> interface which allows the executor to determine when to run them based on notifications sent by the message store and the aggregated stream.</p> <p>Given our objects from the previous chapters, we can use the executor like this:</p> <pre><code>from depeche_db import Executor\n\nexecutor = Executor(db_dsn=DB_DSN)\nexecutor.register(aggregated_stream.projector)\nexecutor.register(subscription.runner)\n\n# this will run until stopped via SIGINT etc\nexecutor.run()\n</code></pre> <p>You can run multiple instances of the same executor on any number of machines, as long as they can talk to the same PostgreSQL database.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Install from PyPI using your favorite package manager</p> <pre><code>pip install depeche-db\n# OR\npoetry add depeche-db\n</code></pre>"},{"location":"getting-started/installation/#optional-run-a-postgresql-database","title":"Optional: Run a PostgreSQL database","text":"<pre><code># docker-compose.yml\nversion: '3'\n\nservices:\n  db_dev:\n    image: 'postgres:14.5'\n    environment:\n      POSTGRES_USER: demo\n      POSTGRES_PASSWORD: demo\n      POSTGRES_DB: demo\n    ports:\n      - 4888:5432\n    restart: unless-stopped\n</code></pre> <pre><code>docker compose up -d\n</code></pre>"}]}