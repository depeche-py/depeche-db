{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Depeche DB","text":"<p>A library for building event-based systems on top of PostgreSQL</p>"},{"location":"#features","title":"Features","text":"<p>Depeche DB has two main parts:</p>"},{"location":"#message-store","title":"Message store","text":"<ul> <li>store &amp; read messages</li> <li>multiple streams</li> <li>optimistic concurrency control</li> </ul>"},{"location":"#subscriptions","title":"Subscriptions","text":"<ul> <li>aggregate messages from multiple streams</li> <li>subscribe to aggregated stream</li> <li>no polling (uses PostgreSQL <code>NOTIFY</code>/<code>LISTEN</code>)</li> <li>tools for keeping track of subscription state</li> <li>concurrent processing (aggregated streams can be partioned)</li> </ul>"},{"location":"#background","title":"Background","text":"<p>Depeche DB is obviously influenced by message stores like EventStoreDB and takes some inspiration from Kafka. More inspiration is taken from Marten, a project that implements an event store on top of PostgreSQL. Another strong influence has been the Message DB project which is pretty similar on the implementation side.</p>"},{"location":"concepts/data-model/","title":"Data model","text":""},{"location":"concepts/data-model/#messages","title":"Messages","text":"<p>The smallest unit of storage is a message. The content of messages is transparent to Depeche DB. Every message needs two basic properties so that it can be stored and processed: An <code>ID</code> of type UUID and a timestamp.</p>"},{"location":"concepts/data-model/#streams","title":"Streams","text":"<p>A stream has a name and contains messages. The message store can contain as many streams as you need.</p> <p>flowchart TB     subgraph stream-1     msg11 --&gt; msg12 --&gt; msg13     end  flowchart TB     subgraph stream-2     msg21 --&gt; msg22     end </p> <p>Messages in a stream are ordered. Their position within the stream is called <code>version</code>. The <code>version</code>s in a stream do not have gaps and are unique.</p> <p>Usually, a stream will be relatively small because it only contains messages concerning a single domain object.</p>"},{"location":"concepts/data-model/#message-store","title":"Message store","text":"<p>The message store contains multiple streams. Messages are assigned a global position when they are added to a stream. This global position is unique but there can be gaps (when transactions are rolled back). For messages <code>a</code> and <code>b</code> within the same stream, it is guaranteed that if <code>a.version &lt; b.version</code> then <code>a.global_position &lt; b.global_position</code> and vice versa.</p>"},{"location":"concepts/data-model/#aggregated-streams","title":"Aggregated streams","text":"<p>Because of the high granularity of the streams, it is not pratical to subscribe to them directly. That is why there is the notion of an aggregated stream which aggregates the messages from several streams.</p> <p>Aggregated streams consist of partitions. Messages from the origin streams can be assigned to partitions by a user defined function. When a message is added to a partition within an aggregated stream, it is not copied but linked to. Thus, aggregated streams are rather light-weight on the storage size.</p> <p>Given these streams: flowchart TB     subgraph stream-1     msg11 --&gt; msg12 --&gt; msg13     style msg11 fill:#feaaaa     style msg12 fill:#feaaaa     style msg13 fill:#feaaaa     end     subgraph stream-2     msg21 --&gt; msg22     style msg21 fill:#aafeaa     style msg22 fill:#aafeaa     end     subgraph stream-3     msg31 --&gt; msg32     style msg31 fill:#aaaafe     style msg32 fill:#aaaafe     end </p> <p>An aggregated stream could look like this: flowchart TB     subgraph partition-1       p11 --&gt; p12 --&gt; p31 --&gt; p13 --&gt; p32       style p11 fill:#feaaaa       style p12 fill:#feaaaa       style p13 fill:#feaaaa       style p31 fill:#aaaafe       style p32 fill:#aaaafe     end  flowchart TB     subgraph partition-2       p21 --&gt; p22       style p21 fill:#aafeaa       style p22 fill:#aafeaa     end </p> <p>Within a partition, messages are ordered by their global position in the message store. Messages are assigned a unique &amp; gapless <code>position</code> within their partition.</p> <p>Please note that this example partitioning (which used the stream name as the partition key) is only one possibility. Users can define how messages are partitioned. Furthermore, you can create multiple aggregated streams which contain the same or overlapping sets of messages with different partitioning strategies. This allows you to tailor the aggregated streams - more specifically their ordering and possible parallelism - to the needs of different consumers.</p>"},{"location":"concepts/subscriptions/","title":"Subscriptions","text":"<p>Given an aggregated stream, we can create a number of subscriptions on it which are discriminated by a name. A subscription works roughly like this:</p> <ol> <li>Wait for a trigger</li> <li>Get the current state (pairs of partition number &amp; last processed position)</li> <li>For each partition that has messages after the known position (ordered by the    message time of the oldest unprocessed message)<ol> <li>Try to acquire a lock (subscription group name, partition)</li> <li>Re-validate the state (a parallel process might have advanced it already)</li> <li>Process the message(s) (depending on the batch size)</li> <li>Update the state with the new position</li> <li>Release the lock</li> </ol> </li> </ol> <p>This algorithm allows us to process messages concurrently while still honoring the ordering guarantees within a partition because only one instance can process messages from any given partition at any time. It can be described as an instance of the competing consumers pattern.</p> <p>The ordering done in step 3 is not necessary to keep any of the guarantees but helps to keep up with the expectation that messages in the system are processed roughly in the order given by their message time. It also helps with fairness, because it will favor processing older messages first.</p> <p>The order of steps 3c and 3d makes this a \"at least once\" delivery system, because the message is processed before the new position is recorded.</p>"},{"location":"concepts/subscriptions/#services-required-by-subscriptions","title":"Services required by subscriptions","text":"<p>A subscription requires two services provided to it:</p> <ol> <li>Subscription state storage</li> <li>(Distributed) Locks</li> </ol> <p>Both of the services have a default implementation but it may be a good idea to check if you should customize/replace it to your needs. The interfaces of both services are pretty simple.</p>"},{"location":"concepts/subscriptions/#state","title":"State","text":"<p>The best location for the subscription state is - especially in the context of transactionally safe storage - close to the state that is altered in the course of the application code handling messages.</p> <p>If you can manage to get the subscription state updates within the same transaction as the application state, you essentially implemented a \"exactly-once\" message processing mechanism.</p>"},{"location":"concepts/subscriptions/#locks","title":"Locks","text":"<p>The default locking provider uses again PostgreSQL, namely advisory locks through the <code>pals</code> library. If for some reason your system is not actually distributed to multiple machines, you could replace if with a simpler &amp; faster locking system. Or maybe you already have a distributed locking infrastructure, then it might be worth a look to use it here too.</p>"},{"location":"generated/output/getting-started-aggregated-stream/","title":"Aggregated stream","text":"<p>We will use the same message store as in the previous chapter here, but we will create a new set of streams within it:</p> <pre><code>import random\n\nfor _ in range(20):\n    n = random.randint(0, 200)\n    stream = f\"aggregate-me-{n % 5}\"\n    message_store.write(stream=stream, message=EventA(num=n))\n</code></pre> <p>For our aggregated stream, we need to prepare a partition function (or rather class).</p> <pre><code>from depeche_db import AggregatedStream, StoredMessage\n\n\nclass NumMessagePartitioner:\n    def get_partition(self, message: StoredMessage[EventA | EventB]) -&gt; int:\n        if isinstance(message.message, EventA):\n            return message.message.num % 3\n        return 0\n</code></pre> <p>Now we can put together the aggregated stream.</p> <pre><code>aggregated_stream = AggregatedStream[EventA | EventB](\n    name=\"example_docs_aggregate_me2\",\n    store=message_store,\n    partitioner=NumMessagePartitioner(),\n    stream_wildcards=[\"aggregate-me-%\"],\n)\naggregated_stream.projector.update_full()\n</code></pre> <p>Whenever we call <code>update_full</code>, all new messages in the origin streams will be appended to the relevant partition of the aggregated stream in the right order. We will not have to call this manually though. We can use the <code>Executor</code> to do it for us.</p> <p>Usually, we do not read the aggregated stream directly, but we would use a subscription to consume it. We will get to that in the next chapter.</p>"},{"location":"generated/output/getting-started-subscription/","title":"Subscription","text":"<p>Given the aggregated stream from the previous chapter, we can put together a subscription.</p> <pre><code>from depeche_db import Subscription\n\nsubscription = Subscription(\n    name=\"sub_example_docs_aggregate_me2\",\n    stream=aggregated_stream,\n)\n</code></pre> <p>You can read from a subscription directly (but you would probably want to use a <code>SubscriptionHandler</code> as shown next). The emitted message is wrapped in a <code>SubscriptionMessage</code> object which contains the metadata about the message in the context of the subscription/aggregated stream.</p> <pre><code>for message in subscription.get_next_messages(count=1):\n    print(message)\n#  SubscriptionMessage(\n#      partition=2,\n#      position=0,\n#      stored_message=StoredMessage(\n#          message_id=UUID(\"1f804185-e63d-462e-b996-d6f16e5ff8af\"),\n#          stream=\"aggregate-me-1\",\n#          version=1,\n#          message=EventA(\n#              event_id=UUID(\"1f804185-e63d-462e-b996-d6f16e5ff8af\"),\n#              happened_at=datetime.datetime(2023, 10, 5, 20, 3, 26, 658725),\n#              num=176,\n#          ),\n#          global_position=4,\n#      ),\n#  )\n</code></pre> <p>In order to continously handle messages on a subscription we would use the <code>SubscriptionHandler</code>:</p> <pre><code>from depeche_db import SubscriptionMessage\n\n\n@subscription.handler.register\ndef handle_event_a(msg: SubscriptionMessage[EventA]):\n    real_message = msg.stored_message.message\n    print(f\"num={real_message.num} (partition {msg.partition} at {msg.position})\")\n</code></pre> <p>You can register multiple handlers for different message types. The handled message types must not overlap. Given your event type <code>E</code>, you can request <code>SubscriptionMessage[E]</code>, <code>StoredMessage[E]</code> or <code>E</code> as the type of the argument to the handler by using type hints.</p> <pre><code>subscription.handler.run_once()\n#  num=111 (partition 0 at 0)\n#  num=199 (partition 1 at 0)\n#  num=166 (partition 1 at 1)\n#  num=0 (partition 0 at 1)\n#  num=152 (partition 2 at 1) # we already saw 2:0 above!\n#  num=172 (partition 1 at 2)\n#  num=12 (partition 0 at 2)\n#  ...\n</code></pre> <p>Running <code>run_once</code> will read the unprocessed messages from the subscription and call the registered handlers (if any).</p> <p>In a real application, we would not call <code>run_once</code> directly, but we would use the <code>Executor</code> to do it for us.</p>"},{"location":"generated/output/getting-started-write-read/","title":"Writing &amp; reading messages","text":"<p>First, create a SQLAlchemy engine with your database connection: <pre><code>from sqlalchemy import create_engine\n\nDB_DSN = \"postgresql://depeche:depeche@localhost:4888/depeche_demo\"\ndb_engine = create_engine(DB_DSN)\n</code></pre></p> <p>Then we define our message types using pydantic. Using pydantic is optional, but it makes serialization straightforward. <pre><code>from datetime import datetime\nfrom uuid import UUID, uuid4\n\nimport pydantic\n\nfrom depeche_db import MessageProtocol\n\n\nclass MyEvent(pydantic.BaseModel, MessageProtocol):\n    event_id: UUID = pydantic.Field(default_factory=uuid4)\n    happened_at: datetime = pydantic.Field(default_factory=datetime.utcnow)\n\n    def get_message_id(self) -&gt; UUID:\n        return self.event_id\n\n    def get_message_time(self) -&gt; datetime:\n        return self.happened_at\n\n\nclass EventA(MyEvent):\n    num: int\n\n\nclass EventB(MyEvent):\n    text: str\n</code></pre></p> <p>Now we are ready to create our message store. This will create a new table <code>example_basic_messages</code> when it is called the first time. <pre><code>from depeche_db import MessageStore\nfrom depeche_db.tools import PydanticMessageSerializer\n\nmessage_store = MessageStore[EventA | EventB](\n    name=\"example_docs2\",\n    engine=db_engine,\n    serializer=PydanticMessageSerializer(EventA | EventB),\n)\n</code></pre></p> <p>Now we write an event to the stream</p> <pre><code>stream = f\"stream-{uuid4()}\"\n\nresult = message_store.write(stream=stream, message=EventA(num=42))\nprint(result)\n#  MessagePosition(stream='stream-&lt;uuid&gt;', version=1, global_position=1)\n</code></pre> <p>Here is how we can read the messages: <pre><code>print(next(message_store.read(stream)))\n#  StoredMessage(\n#    message_id=UUID('&lt;uuid&gt;'),\n#    stream='stream-&lt;uuid&gt;',\n#    version=1,\n#    message=EventA(\n#      event_id=UUID('&lt;uuid&gt;'),\n#      num=42,\n#      happened_at=datetime.datetime(...)\n#    ),\n#    global_position=1\n#  )\n</code></pre></p> <p>Please note that when reading, the original message is wrapped in a <code>StoredMessage</code> object, which contains the metadata about the message.</p> <p>When we write, we can pass an <code>expected_version</code> parameter, which gives us optimistic concurrency control. <pre><code># this fails because the expected version is 0, but the stream already has a message\nmessage_store.write(stream=stream, message=EventA(num=23), expected_version=0)\n# this is fine, because we expect the right version\nmessage_store.write(stream=stream, message=EventA(num=23), expected_version=1)\n</code></pre></p>"},{"location":"getting-started/executor/","title":"Executor","text":"<p>Executing the regular jobs of</p> <ul> <li>updating aggregated streams</li> <li>running handlers on subscriptions</li> </ul> <p>by hand is cumbersome. <code>StreamProjector</code> and <code>SubscriptionHandler</code> both implement the <code>RunOnNotification</code> interface which allows the executor to determine when to run them based on notifications sent by the message store and the aggregated stream.</p> <p>Given our objects from the previous chapters, we can use the executor like this:</p> <pre><code>from depeche_db import Executor\n\nexecutor = Executor(db_dsn=DB_DSN)\nexecutor.register(aggregated_stream.projector)\nexecutor.register(subscription.handler)\n\n# this will run until stopped via SIGINT etc\nexecutor.run()\n</code></pre> <p>You can run multiple instances of the same executor on any number of machines, as long as they can talk to the same PostgreSQL database.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Install from PyPI using your favorite package manager</p> <pre><code>pip install depeche-db\n# OR\npoetry add depeche-db\n</code></pre>"},{"location":"getting-started/installation/#optional-run-a-postgresql-database","title":"Optional: Run a PostgreSQL database","text":"<pre><code># docker-compose.yml\nversion: '3'\n\nservices:\n  db_dev:\n    image: 'postgres:14.5'\n    environment:\n      POSTGRES_USER: demo\n      POSTGRES_PASSWORD: demo\n      POSTGRES_DB: demo\n    ports:\n      - 4888:5432\n    restart: unless-stopped\n</code></pre> <pre><code>docker compose up -d\n</code></pre>"}]}